\documentclass[12pt, letterpaper, twoside]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{amsmath,amssymb}
\usepackage{indentfirst}
\usepackage{hyperref}

%opening
\title{Логистическая регрессия для решения задач классификации}
\author{Вихляев Егор, ММТ-2}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Описание проблемы, для решения которой используется данная модель}

Логистическая регрессия - это статистический метод, который обычно используется для решения задач классификации, а не задач регрессии, несмотря на свое название.

Вот несколько примеров проблем, для решения которых часто используется логистическая регрессия:

Бинарная классификация, где цель - разделить объекты на два класса. Например, определение, является ли письмо спамом или не спамом, определение, болен ли пациент определенной болезнью (да/нет), и многие другие подобные задачи.

Многоклассовая классификация, где объекты должны быть отнесены к одному из нескольких классов. Это делается с использованием множества бинарных классификаторов или методов, таких как "один против всех" или "многие против многих".

Прогнозирование вероятности события. Например, вероятность клиента купить продукт, вероятность ухода клиента из компании, вероятность прохождения студентом экзамена и т. д.

Медицинская диагностика, где определяется наличие или отсутствие болезни на основе медицинских показателей и тестов.

Финансовый анализ. В финансовой сфере логистическая регрессия может использоваться для определения вероятности дефолта заемщика, анализа кредитоспособности, антифрод-анализа и других задач.

Логистическая регрессия может применяться для анализа данных о клиентах, определения, склонны ли они к покупке определенных товаров или услуг, или к каким-либо другим действиям.

Логистическая регрессия хорошо подходит для задач, где требуется оценивать вероятность принадлежности объекта к классу, и она широко используется во многих областях, где необходима классификация и прогнозирование с учетом вероятностей.

\section{Концептуальная постановка}

На практике логистическая регрессия используется для решения задач классификации с линейно-разделяемыми классами. При этом, предполагается, что зависимая переменная принимает два значения и имеет биномиальное распределение.

\section{Математическая постановка (постановка задачи восстановления логистической регрессии)}

Пусть дана выборка $(X,Y)$, где $X = \begin{bmatrix}
	X_{1} \\
	... \\
	X_{m}
\end{bmatrix}$ -- набор объектов, $Y = \begin{bmatrix}
Y_{1} \\
... \\
Y_{m}
\end{bmatrix}$ - ответы на этих объектах. $x_{i} \in \mathbb{R}^{n}$. $y_{i} \in {0;1}$ - один из двух классов: либо 0, либо 1. \\

Нужно построить функцию $f: x_{i}  \leadsto \widetilde{y_{i}} \approx y_{i}$. Логистическая регрессия задает структуру данной функции. \\

$\widetilde{y_{i}} = f(x_{i}; w, b) = \sigma(w^{T}x_{i}+b)$ - сигмоида от суммы скалярного прозведения и смещения. $w \in \mathbb{R}^{n}$ - вещ. вектор, $b \in \mathbb{R}$ - вещ. число (смещение). \\

Логистическая регрессия для объекта $x_{i}$ получает предсказание в два шага: \\
1. $z = (- w^{T} -) \begin{bmatrix}
	| \\
	x_{i} \\
	|
\end{bmatrix} + b \in \mathbb{R}$ - получаем из вектора число.\\
2. Полученное число преобразуем в меру активации: $a = \sigma(z) = \frac{1}{1+e^{-z}}$ - логистическая функция или сигмоида. \\

Таким образом мы задали модель и способ вычисления предсказания. Поскольку в способе есть два неизвестных параметра $w$ и $b$, то подберем их через функцию потерь: $$L_{i}(\widetilde{y_{i}}) = -y_{i}log(\widetilde{y_{i}}) - (1-y_{i})log(1-\widetilde{y_{i}}).$$ На выборке $(X, Y): L = \frac{1}{m}\sum_{i=1}^{m}L_{i}(\widetilde{y_{i}}) \rightarrow min(w,b) = \frac{1}{m}\sum_{i=1}^{m}L_{i}(\sigma(w^{T}x_{i}+b)) \rightarrow min(w,b)$. Задача свелась к минимизации функции потерь. Минимизируем градиентным спуском. \\

$w_{j+1} := w_{j} - \alpha\frac{\partial L(w_{j}, b_{j})}{\partial w},$ где $w_{j}$ - параметр на текущем шаге, $\alpha$ - шаг градиентного спуска, $\frac{\partial L(w_{j}, b_{j})}{\partial w}$ - градиент. Аналогично для параметра $b$. \\

Поскольку $L_{i}(w,b) = L_{i}(a(z(w,b)))$ - сложная функция, то ищем ее частную производную следующим образом: $$\frac{\partial L(w_{j}, b_{j})}{\partial w} = \frac{\partial L_{i}}{\partial a}\frac{\partial a}{\partial z}\frac{\partial z}{\partial w}.$$ Тогда: $$\frac{\partial L_{i}}{\partial a} = -\frac{y_{i}}{a} + \frac{1-y_{i}}{1-a} = \frac{a-y_{i}}{a(1-a)},$$ $$\frac{\partial a}{\partial z} = a(1-a),$$ $$\frac{\partial z}{\partial w} = \frac{\partial (w^{T}x_{i}+b)}{\partial w} = \begin{bmatrix}
	| \\
	x_{i} \\
	|
\end{bmatrix}.$$ Таким образом: $$\frac{\partial L_{i}}{\partial w} = \frac{a-y_{i}}{a(1-a)}*a(1-a)*x_{i} = (a-y_{i}x_{i}),$$ при этом, аналогично, $$\frac{\partial L_{i}}{\partial b} = a-y_{i}.$$

\end{document}
